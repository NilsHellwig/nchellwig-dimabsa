{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "509021e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBTASKS = [3, 2]\n",
    "LANGUAGES = [\"eng\", \"jpn\", \"rus\", \"tat\", \"ukr\", \"zho\"]\n",
    "DOMAINS = [\"restaurant\", \"laptop\", \"hotel\", \"finance\"]\n",
    "STRATEGY = \"train_split\"  # \"pred_dev\" oder \"train_split\"\n",
    "RUN_SEED = 0 # allgemeine Seed f체r Reproduzierbarkeit\n",
    "N_SPLITS = 5  # Anzahl der 80/20 Splits f체r train_split\n",
    "EPOCHS = [5, 10, 15]\n",
    "LLM = \"unsloth/gemma-3-4b-it-bnb-4bit\"  \n",
    "N_RUNS = 5 # Wie oft wurde prompt ausgef체hrt bei self-consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4439be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from evaluate import evaluate_predictions\n",
    "import statistics\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42514aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_sc_guided': {'TP': 1013.7128240313489,\n",
       "  'FP': 675,\n",
       "  'FN': 655,\n",
       "  'cPrecision': 0.5832639954150454,\n",
       "  'cRecall': 0.5900540302860006,\n",
       "  'cF1': 0.5866393657588825},\n",
       " 'no_sc_no_guided': {'TP': 1006.5833898320235,\n",
       "  'FP': 682,\n",
       "  'FN': 662,\n",
       "  'cPrecision': 0.5791619043912678,\n",
       "  'cRecall': 0.5859041850011778,\n",
       "  'cF1': 0.582513535782421},\n",
       " 'sc_guided': {'TP': 1022.8177567307357,\n",
       "  'FP': 634,\n",
       "  'FN': 648,\n",
       "  'cPrecision': 0.6002451624006665,\n",
       "  'cRecall': 0.5953537582833153,\n",
       "  'cF1': 0.5977894545474784},\n",
       " 'sc_no_guided': {'TP': 1014.1087221886793,\n",
       "  'FP': 635,\n",
       "  'FN': 657,\n",
       "  'cPrecision': 0.5979414635546458,\n",
       "  'cRecall': 0.5902844715882883,\n",
       "  'cF1': 0.5940882965370119}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "\n",
    "def load_predictions(subtask, language, domain, split_idx, strategy=\"train_split\", guidance=True, self_consistency=True, run_idx=0):\n",
    "    llm_name_formatted = LLM.replace(\"/\", \"_\")\n",
    "\n",
    "    guidance_str = \"with_guidance\" if guidance else \"no_guidance\"\n",
    "    temp_str = \"_temp0.8\" if self_consistency else \"_temp0\"\n",
    "    run_str = f\"_run{run_idx}\" if self_consistency else \"\"\n",
    "    split_idx_str = f\"_{split_idx}\" if strategy == \"train_split\" else \"\"\n",
    "\n",
    "    path = f\"results/results_{strategy}/{llm_name_formatted}/{subtask}_{language}_{domain}_{RUN_SEED}{split_idx_str}{temp_str}_{guidance_str}{run_str}.jsonl\"\n",
    "\n",
    "    predictions = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            predictions.append(data)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def load_ground_truth(subtask, language, domain):\n",
    "    # task-dataset/track_a/subtask_2/eng/eng_laptop_train_alltasks.jsonl\n",
    "    path = f\"task-dataset/track_a/subtask_{subtask}/{language}/{language}_{domain}_train_alltasks.jsonl\"\n",
    "    ground_truth = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            ground_truth.append(data)\n",
    "    return ground_truth\n",
    "\n",
    "\n",
    "def filter_predictions(predictions, ground_truth):\n",
    "    labels_filtered = []\n",
    "\n",
    "    preds_dict = {pred['ID']: pred for pred in predictions}\n",
    "    for label in ground_truth:\n",
    "        if label['ID'] in preds_dict:\n",
    "            labels_filtered.append(label)\n",
    "    return labels_filtered\n",
    "\n",
    "\n",
    "def merge_predictions(predictions, subtask):\n",
    "    counter = defaultdict(list)\n",
    "\n",
    "    # Annahme: alle Runs haben dieselbe ID\n",
    "    final_id = predictions[0][\"ID\"]\n",
    "\n",
    "    for run in predictions:\n",
    "        for quad in run[\"Quadruplet\"]:\n",
    "            # Key abh채ngig vom Subtask\n",
    "            if subtask == 3:\n",
    "                key = (quad[\"Aspect\"], quad[\"Category\"], quad[\"Opinion\"])\n",
    "            else:  # subtask == 2\n",
    "                key = (quad[\"Category\"], quad[\"Opinion\"])\n",
    "\n",
    "            # Valence/Arousal parsen\n",
    "            valence_str, arousal_str = quad[\"VA\"].split(\"#\")\n",
    "            valence = float(valence_str)\n",
    "            arousal = float(arousal_str)\n",
    "\n",
    "            counter[key].append((valence, arousal))\n",
    "\n",
    "    merged = []\n",
    "\n",
    "    # Verarbeitung der aggregierten Quadruplets\n",
    "    for key, values in counter.items():\n",
    "        if len(values) >= 3:\n",
    "            mean_valence = statistics.mean(v[0] for v in values)\n",
    "            mean_arousal = statistics.mean(v[1] for v in values)\n",
    "\n",
    "            if subtask == 3:\n",
    "                aspect, category, opinion = key\n",
    "                merged.append({\n",
    "                    \"Aspect\": aspect,\n",
    "                    \"Category\": category,\n",
    "                    \"Opinion\": opinion,\n",
    "                    \"VA\": f\"{mean_valence:.2f}#{mean_arousal:.2f}\"\n",
    "                })\n",
    "            else:\n",
    "                category, opinion = key\n",
    "                merged.append({\n",
    "                    \"Category\": category,\n",
    "                    \"Opinion\": opinion,\n",
    "                    \"VA\": f\"{mean_valence:.2f}#{mean_arousal:.2f}\"\n",
    "                })\n",
    "\n",
    "    # Finaler Output mit passendem Key\n",
    "    if subtask == 2:\n",
    "        return {\n",
    "            \"ID\": final_id,\n",
    "            \"Triplet\": merged\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"ID\": final_id,\n",
    "            \"Quadruplet\": merged\n",
    "        }\n",
    "\n",
    "\n",
    "def get_performance(language, domain, subtask, strategy):\n",
    "    labels = load_ground_truth(subtask, language, domain)\n",
    "\n",
    "    results = []\n",
    "    n_splits = 1 if strategy == \"train_split\" else 1\n",
    "\n",
    "    for split_idx in range(n_splits):\n",
    "        # 1a\n",
    "        preds_no_sc_guided = load_predictions(\n",
    "            subtask, language, domain, split_idx=split_idx, strategy=STRATEGY, guidance=True, self_consistency=False)\n",
    "        # 1b\n",
    "        preds_no_sc_no_guided = load_predictions(\n",
    "            subtask, language, domain, split_idx=split_idx, strategy=STRATEGY, guidance=False, self_consistency=False)\n",
    "        # 2a\n",
    "        preds_sc_guided = []\n",
    "\n",
    "        preds_0 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=True, self_consistency=True, run_idx=0)\n",
    "        preds_1 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=True, self_consistency=True, run_idx=1)\n",
    "        preds_2 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=True, self_consistency=True, run_idx=2)\n",
    "        preds_3 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=True, self_consistency=True, run_idx=3)\n",
    "        preds_4 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=True, self_consistency=True, run_idx=4)\n",
    "        for k in range(len(preds_0)):\n",
    "            merged_quads = merge_predictions(\n",
    "                [preds_0[k], preds_1[k], preds_2[k], preds_3[k], preds_4[k]], subtask=subtask)\n",
    "            preds_sc_guided.append(merged_quads)\n",
    "\n",
    "        # 2b\n",
    "        preds_sc_no_guided = []\n",
    "\n",
    "        preds_0 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=False, self_consistency=True, run_idx=0)\n",
    "        preds_1 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=False, self_consistency=True, run_idx=1)\n",
    "        preds_2 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=False, self_consistency=True, run_idx=2)\n",
    "        preds_3 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=False, self_consistency=True, run_idx=3)\n",
    "        preds_4 = load_predictions(subtask, language, domain, split_idx=split_idx,\n",
    "                                   strategy=STRATEGY, guidance=False, self_consistency=True, run_idx=4)\n",
    "        for k in range(len(preds_0)):\n",
    "            merged_quads = merge_predictions(\n",
    "                [preds_0[k], preds_1[k], preds_2[k], preds_3[k], preds_4[k]], subtask=subtask)\n",
    "            preds_sc_no_guided.append(merged_quads)\n",
    "\n",
    "\n",
    "        labels_filtered = filter_predictions(preds_no_sc_guided, labels)\n",
    "        \n",
    "        results.append({\n",
    "            \"no_sc_guided\": evaluate_predictions(labels_filtered, preds_no_sc_guided, task=subtask),\n",
    "            \"no_sc_no_guided\": evaluate_predictions(labels_filtered, preds_no_sc_no_guided, task=subtask),\n",
    "            \"sc_guided\": evaluate_predictions(labels_filtered, preds_sc_guided, task=subtask),\n",
    "            \"sc_no_guided\": evaluate_predictions(labels_filtered, preds_sc_no_guided, task=subtask),\n",
    "        })\n",
    "    \n",
    "    # calculate average over splits\n",
    "    if strategy == \"train_split\":\n",
    "        avg_results = {}\n",
    "        for key in results[0].keys():\n",
    "            avg_results[key] = {}\n",
    "            for metric in results[0][key].keys():\n",
    "                avg_results[key][metric] = statistics.mean(\n",
    "                    result[key][metric] for result in results)\n",
    "        return avg_results\n",
    "    else:    \n",
    "        return results[0], preds_sc_no_guided, preds_sc_guided\n",
    "        \n",
    "get_performance(\"zho\", \"restaurant\", 3, STRATEGY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
